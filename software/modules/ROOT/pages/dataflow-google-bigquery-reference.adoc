= Google BigQuery connection reference
:last_updated: 03/11/2022
:experimental:
:linkattrs:
:page-aliases: /data-integrate/dataflow/dataflow-google-bigquery-reference.adoc

Learn about the fields used to create a Google BigQuery connection with ThoughtSpot DataFlow.

Here is a list of the fields for a Google BigQuery connection in ThoughtSpot DataFlow.
You need specific information to establish a seamless and secure connection.

[#connection-properties]
== Connection properties
[#dataflow-google-bigquery-conn-connection-name]
Connection name:: Name your connection. Mandatory field.
Example:;; GoogleBigQueryConnection
[#dataflow-google-bigquery-conn-connection-type]
Connection type:: Choose the Google BigQuery connection type. Mandatory field.
Example:;; Google BigQuery
[#dataflow-google-bigquery-conn-project-id]
Project id:: The identification number given to particular project, always unique. Mandatory field.
Example:;; myproject-1234
[#dataflow-google-bigquery-conn-authentication-type]
Authentication type:: It can be either Service Account or Access Tokens. Mandatory field.
Example:;; Service Account
Valid Values:;; Service Account, Access Token
Default:;; Service Account
[#dataflow-google-bigquery-conn-service-account-key-access-token]
Service Account key/Access Token:: Provide the Service Account key when authentication type is selected as Service account and token when access token is selected as authentication type. Mandatory field.
Example:;; ABCDEFGH245HIJK
[#dataflow-google-bigquery-conn-query-priority]
Query priority:: Specify the time duration to run the query and it can be either Interactive or Batch. Optional field.
Example:;; BATCH
Valid Values:;; INTERACTIVE, BATCH
Default:;; BATCH
Other notes:;; In *Advanced configuration*
[#dataflow-google-bigquery-cloud-storage-location]
Cloud storage location:: Provide the GCS bucket and object location details.
Example:;; gs://GCS bucket/object location
Other notes:;; In *Advanced configuration*
[#dataflow-google-bigquery-conn-staging-database]
Staging database:: Specify the name of the staging database. Optional field.
Example:;; testdb
Other notes:;; In *Advanced configuration*
[#dataflow-google-bigquery-conn-staging-database-project-id]
Staging database project ID:: Specify the unique identification number given to staging database. Optional field.
Example:;; phrasal-indexer-12345
Other notes:;; In *Advanced configuration*
[#dataflow-google-bigquery-conn-use-proxy]
Use proxy:: If required, to use a proxy, select the checkbox. Use Proxy and provide the details Optional field.
Other notes:;; In *Advanced configuration*
[#dataflow-google-bigquery-conn-host]
Host:: Specify the hostname or the IP address of the BigQuery system.
Optional field. +
 For proxy authentication only.
 Example:;; www.example.com
[#dataflow-google-bigquery-conn-port]
 Port:: Specify the port associated to the BigQuery system.
Optional field. +
 For proxy authentication only.
 Example:;; 1234
[#dataflow-google-bigquery-conn-protocol]
 Protocol:: It can be either http or https.
Optional field. +
 For proxy authentication only.
 Example:;; http
 Valid Values:;; http, https
 Default:;; http

////
[#dataflow-google-bigquery-conn-jdbc-options]
include::partial$dataflow/jdbc-options.adoc[]
////

[#sync-properties]
== Sync properties
[#dataflow-google-bigquery-sync-column-delimiter]
Column delimiter:: Specify the column delimiter character. Mandatory field.
Example:;; 1
Valid Values:;; Any printable ASCII character, or the decimal value for an ASCII character.
Default:;; The delimiter specified in sync
[#dataflow-google-bigquery-sync-enclosing-character]
Enclosing character:: Specify if the text columns in the source data needs to be enclosed in quotes. Optional field.
Example:;; DOUBLE
Valid Values:;; DOUBLE, SINGLE, NULL
Default:;; SINGLE
[#dataflow-google-bigquery-sync-escape-character]
Escape character:: Specify the escape character if using a text qualifier in the source data. Optional field.
Example:;; \"
Valid Values:;; \\, any ASCII character
Default:;; \"
[#dataflow-google-bigquery-sync-fetch-size]
Fetch size::
Specify the number of rows to fetch at one time, and process in memory.
To fetch all rows, specify 0 rows. Optional field.
Example:;; 1000
Valid Values:;; 1000, 10, 100.
100000, any numeric value
Default:;; 10
[#dataflow-google-bigquery-sync-allow-large-resultset]
Allow large resultset:: If enabled, allows query results that are larger in size. Optional field.
Example:;; FALSE
Valid Values:;; TRUE
Default:;; FALSE
[#dataflow-google-bigquery-sync-max-ignored-rows]
include::partial$dataflow/max-ignored-rows.adoc[]

[#dataflow-google-bigquery-sync-ts-load-options]
include::partial$dataflow/ts-load-options.adoc[]

'''
> **Related information**
>
> * xref:dataflow-google-bigquery-add.adoc[Add a connection]
> * xref:dataflow-google-bigquery-sync.adoc[Sync data]
