= Data caching
:last_updated: 06/29/2021
:experimental:
:linkattrs:
:description: ThoughtSpot caches data as relational tables in memory.

ThoughtSpot does all analysis against data in memory to help achieve fast results across millions and billions of records of data.

ThoughtSpot caches data as relational tables in memory.
You can source tables from different data sources and join them together.
ThoughtSpot has several approaches for getting data into the cluster.

image::data-caching-architecture.png[Data caching architecture diagram. A Data sources icon has an arrow pointing to the following 4 boxes: ThoughtSpot Embrace, ThoughtSpot DataFlow, JDBC/ODBC, and tsload command line utility. There is a Data engineers icon under the 4 boxes, with an arrow pointing up to the 4 boxes. The 4 boxes have 1 arrow pointing to the ThoughtSpot interface icon. There is an End users icon under the ThoughtSpot interface icon, with an arrow pointing up to the ThoughtSpot interface icon.]

NOTE: For cases where your company stores data externally, use connections to access and query your data. To cache your data within ThoughtSpot, you can load it directly using DataFlow, or the `tsload` command-line utility. JDBC and ODBC drivers are also available.

== ThoughtSpot connections

If your company stores source data externally in data warehouses, you can use xref:connections.adoc[] to directly query that data and use ThoughtSpot analysis and visualization features, without moving the data into ThoughtSpot. While connections cache metadata, they *do not* cache the data itself within ThoughtSpot.

You can connect to the following external databases:

[cols="15,~",frame=none,grid=none]
|===
a| image::logo-redshift.png[Amazon Redshift logo] .^a|xref:connections-redshift.adoc[Amazon Redshift (AWS)]
a| image::logo-synapse.png[Azure Synapse logo]  .^a|xref:connections-synapse.adoc[Azure Synapse]
a|  image::logo-databricks.png[Databricks logo] .^a|xref:connections-databricks.adoc[Databricks]
a|  image::logo-denodo.png[Denodo logo] .^a|xref:connections-denodo.adoc[Denodo]
a|  image::logo-dremio.png[Dremio logo] .^a|xref:connections-dremio.adoc[Dremio]
a| image::logo-gcp.png[Google BigQuery logo] .^a|xref:connections-gbq.adoc[Google Big Query]
a| image::logo-oracle.png[Oracle logo] .^a|xref:connections-adw.adoc[Oracle]
a| image::logo-postgresql.png[PostgreSQL logo] .^a|xref:connections-postgresql.adoc[PostgreSQL]
a| image::logo-presto.png[Presto logo] .^a|xref:connections-presto.adoc[Presto]
a| image::logo-sap.png[SAP HANA logo] .^a|xref:connections-hana.adoc[SAP HANA]
a| image::logo-snowflake.png[Snowflake logo] .^a|xref:connections-snowflake.adoc[Snowflake]
a|  image::logo-starburst.png[Starburst logo] .^a|xref:connections-starburst.adoc[Starburst]
a| image::logo-teradata.png[Teradata logo] .^a|xref:connections-teradata.adoc[Teradata]
a| image::logo-trino.png[Trino logo] .^a|xref:connections-trino.adoc[Trino]

|===

== ThoughtSpot DataFlow

xref:dataflow.adoc[Dataflow] is a capability in ThoughtSpot through which users can easily ingest data into ThoughtSpot from dozens of the most common databases, data warehouses, file sources, and applications. If your company maintains large sources of data externally, you can use DataFlow to easily ingest the relevant information, and use ThoughtSpot's analysis and visualization features. After you configure the scheduled refresh, your analysis features are always up-to-date. DataFlow supports a large number of xref:dataflow-databases.adoc[databases], xref:dataflow-applications.adoc[applications], and xref:dataflow-filesystems.adoc[file systems].

== JDBC and ODBC Drivers

ThoughtSpot provides a xref:jdbc-driver.adoc[JDBC] and xref:odbc.adoc[ODBC] driver that can be used to write data to ThoughtSpot.
This is useful for customers who already have an existing ETL process or tool, and want to extend it to populate the ThoughtSpot cache.

JDBC and ODBC drivers are appropriate under the following circumstances:

* have an ETL load, such as Informatica, SSIS, and so on
* have available resources to create and manage ETL
* have smaller daily loads

== tsload

You can use the xref:tsload-import-csv.adoc[`tsload`] command line tool to bulk load delimited data with very high throughput.
Finally, individual users can upload smaller (< 50MB) spreadsheets or delimited files.

We recommend the `tsload` approach in the following cases:

* initial data load
* JDBC or ODBC drivers are not available
* there are large recurring daily loads
* for higher throughput;
this can add I/O costs

== Choosing a data caching Strategy

The approach you choose depends on your environment and data needs.
There are, of course, tradeoffs between different data caching options.

Many implementations use a variety of approaches.
For example, a solution with a large amount of initial data and smaller daily increments might use `tsload` to load the initial data, and then use the JDBC driver with an ETL tool for incremental loads.

'''
> **Related information**
>
> * xref:connections.adoc[]
> * xref:dataflow.adoc[ThoughtSpot DataFlow]
> * xref:odbc.adoc[ThoughtSpot with ODBC]
> * xref:jdbc-driver.adoc[ThoughtSpot with JDBC]
> * xref:tsload-import-csv.adoc[ThoughtSpot `tsload`]
> * xref:tscli-command-ref.adoc[`tscli` command reference]
> * xref:tsload-api-flags.adoc[`tsload` flag reference]
