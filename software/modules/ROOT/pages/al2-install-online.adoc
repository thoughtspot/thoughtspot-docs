= Install the ThoughtSpot application on online clusters that use Amazon Linux 2
:last_updated: 01/20/2021
:linkattrs:
:page-aliases: /appliance/amazon-linux-2/al2-install-online.adoc
:experimental:

Install ThoughtSpot on Amazon Linux 2 online clusters.

Before starting the install, complete the xref:al2-prerequisites.adoc[pre-installation steps]. If you are using the https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.html[AWS SSM agent^] as an alternative to SSH, you must run the Ansible playbook and all commands on the __SSM console__.

In an online cluster, the hosts can access the public repositories to download the required packages.

Before you build the ThoughtSpot cluster and install the ThoughtSpot application on the hosts, you must run the Ansible playbook. The TS Ansible playbook prepares your clusters in the following manner:

* Ansible installs the required packages: YAML, Python, and R packages; see xref:al2-packages.adoc[Packages installed with ThoughtSpot for Amazon Linux 2].
* It creates and configures local user accounts for ThoughtSpot:
   ** `admin` user has full administrative functionality
   ** `thoughtspot` user can load data in the application
* It installs the ThoughtSpot CLI, `tscli`.
* It configures the ThoughtSpot host nodes:
   ** checks that customization scripts can execute on the nodes
   ** checks that the partitions meet minimum size requirements

The general steps for installing ThoughtSpot on Amazon Linux 2 clusters are:
[cols="5,~",grid=none,frame=none]
|===
| &#10063; | <<configure-ansible,1. Configure the Ansible Playbook>>
| &#10063; | <<run-ansible,2. Run the Ansible Playbook>>
| &#10063; | <<install-thoughtspot,2. Install ThoughtSpot>>
|===

[#configure-ansible]
== Configure the Ansible Playbook

To set up the Ansible, follow these steps:

. Obtain the Ansible tarball by talking to your ThoughtSpot contact. Download it to your local machine.
+
You can download it by running the cp command. For example, if the tarball is in your S3 bucket, run `aws s3 cp s3://bucket_name/path/to/the/tarball ./`.
+
Note that you only need to copy the tarball to one node.
. Unzip the Ansible tarball, to see the following files and directories on your local machine:
+
customize.sh:: This script runs as the last step in the preparation process. You can use it to inject deployment-specific customizations, such as enabling or disabling a corporate proxy, configuring extra SSH keys, installing extra services, and so on.
hosts.sample:: The Ansible inventory file.
prod_image:: This directory contains the ThoughtSpot tools and `tscli`, the ThoughtSpot CLI binary.
README.md:: Basic information for the unzipped file.
rpm_gpg:: This directory contains the https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2rl_verify.html[GPG keys^] that authenticate the public repository.
toolchain:: he tools that are necessary to compile the instructions you define in the Ansible Playbook, the source code, into executables that can run on your device. The toolchain includes a compiler, a linker, and run-time libraries.
ts-new.yaml:: The Ansible Playbook for new installations.
ts-update.yaml:: The Ansible Playbook for updates.
ts.yaml::
yum.repos.d:: This directory contains information about the yum repo used by the cluster.
+
. Copy the Ansible inventory file hosts.sample to hosts.yaml, and using a text editor of your choice, update the file to include your host configuration.
+
Copy the file by running this command: `cp hosts.sample hosts.yaml`.
+
If you are using SSM, you must additionally run a command to replace the `ts_partition_name`, and run a command to create a single partition on the disk mounted under `/export`. Run the following command to replace the ts_partition_name:
+
[source]
----
TS_DISK=disk_name_for_export_partition
  TS_PARTITION_NAME=${TS_DISK}1
sed -i "s/xvda9/$TS_PARTITION_NAME/g" hosts.yaml
----
+
Then run this command to create a single partition on the disk mounted under `/export`:
+
[source]
----
sudo parted -s /dev/$TS_DISK mklabel gpt
sudo parted -s /dev/$TS_DISK mkpart primary xfs 0% 100%
----

*[7.1.1 and later]* is_user_wheel_group::
Specifies if the administrator user should be added to the wheel group. The default is `true`. If you specify `false`, the administrator user is not added to the wheel group.

*[7.1.1 and later]* minimal_sudo_install::
When this is defined, TS disables certain functionality to avoid making additional sudo calls.  This functionality includes the email notification management system, some cluster statistics reporting, and logging of connectivity status between nodes. The default is undefined.

*[7.1.1 and later]* external_sudo_manager::
When this is configured, ThoughtSpot does not make any changes to the sudoers file, such as adding the administrator user. The user is then responsible for ensuring that the administrator user has the ability to run certain elevated privilege commands. The default is undefined.

*[7.1.1 and later]* no_mail_packages::
When this is defined, ThoughtSpot does not install the mail packages `mutt` and `postfix`.  This only applies for online installations. The default is undefined.

*[7.1.1 and later]* skip_sshd_config::
When this is configured, ThoughtSpot does not make any changes to the sshd configuration of the node.  The user must ensure that the MaxSessions value for the administrator user is at least 10. The default is undefined.

*[7.1.1 and later]* skip_yum_update::
When this is defined, the ansible playbook does not attempt to run a blanket yum update to pull the latest packages. The default is undefined.

*[7.1.1 and later]* skip_time_sync_setup::
When this is defined, ThoughtSpot does not configure time synchronization between nodes using `ntp`. The user must configure time synchronization using either `ntp` or `chronyd` themselves. The default is undefined.

hosts:: Add the IP addresses or hostnames of all hosts in the ThoughtSpot cluster.
admin_uid:: The admin user ID parameter. If you are using `ssh` instead of AWS SSM, use the default values. If you are using SSM, the `ssm_user` uses the default value, `1001`. You must choose a new value. Note that the `thoughtspot` user uses `1002`, so you cannot use `1001` or `1002`.
admin-gid:: The admin user group ID. If you are using `ssh` instead of AWS SSM, use the default values. If you are using SSM, the `ssm_user` uses the default value, `1001`. You must choose a new value. Note that the `thoughtspot` user uses `1002`, so you cannot use `1001` or `1002`.
ssh_user:: The `ssh_user` must exist on the ThoughtSpot host, and it must have `sudo` privileges. This user is the same as the `ec2_user`.
+
If you are using AWS SSM instead of ssh, there is no need to fill out this parameter.
ssh_private_key:: Add the private key for `ssh` access to the `hosts.yaml` file. You can use an existing key pair, or generate a new key pair in the Ansible Control server.
Run the following command to verify that the Ansible Control Server can connect to the hosts over `ssh`:
+
[source]
----
ansible -m ping -i hosts.yaml all
----
+
If you are using AWS SSM instead of ssh, there is no need to fill out this parameter or run the above command.
ssh_public_key:: Add the public key to the `ssh authorized_keys` file for each host, and add the private key to the `hosts.yaml` file. You can use an existing key pair, or generate a new key pair in the Ansible Control server.
Run the following command to verify that the Ansible Control Server can connect to the hosts over `ssh`:
+
[source]
----
ansible -m ping -i hosts.yaml all
----
+
If you are using AWS SSM instead of ssh, there is no need to fill out this parameter or run the above command.
extra_admin_ssh_key:: [Optional] An additional or extra key may be required by your security application, such as Qualys, to connect to the hosts.
If you are using AWS SSM instead of ssh, there is no need to fill out this parameter.
http(s)_proxy:: If the hosts must access public repositories through an internal proxy service, provide the proxy information.
This release of ThoughtSpot does not support proxy credentials to authenticate to the proxy service.
ts_partition_name:: The extended name of the ThoughtSpot export partition, such as `/dev/sdb1`.

[#run-ansible]
== Run the Ansible Playbook

Run the Ansible Playbook from your local machine or the SSM console by entering the following command:
[source]
----
ansible-playbook -i hosts.yaml ts.yaml
----

As the Ansible Playbook runs, it will perform these tasks:

. Trigger the installation of xref:al2-packages.adoc[Yum, Python, and R packages]
. Configure the local user accounts that the ThoughtSpot application uses
. Install the ThoughtSpot CLI
. Configure all the nodes in the ThoughtSpot cluster:
  - Format and create export partitions, if they do not exist

After the Ansible Playbook finishes, run the `prepare_disks` script on every node. You *must* run this script as an admin user. Specify the data drives by adding the full device path for all data drives, such as `/dev/sdc`, after the script name. Separate data drives with a space.

. Switch to the admin user, if necessary:
+
[source]
----
su admin
----

. Run the `prepare_disks` script:
+
[source]
----
/usr/local/scaligent/bin/prepare_disks.sh /dev/sdc /dev/sdd
----

Your hosts are now ready for installing the ThoughtSpot application.

[#install-thoughtspot]
== Install the ThoughtSpot cluster and the application

Refer to xref:aws-cluster-install.adoc[Install ThoughtSpot clusters in AWS] for more detailed information on installing the ThoughtSpot cluster.

Follow these general steps to install ThoughtSpot on the prepared hosts:

. Connect to the host as an admin user.
. Download the release artifact from the ThoughtSpot file sharing system.
. Upload the release artifact to the first host.
. Run the `tscli cluster create` command. This script prompts for user input.
. Check the cluster health by running health checks and logging into the application.

'''
> **Related information**
>
> * xref:al2-prerequisites.adoc[Amazon Linux 2 prerequisites]
> * xref:al2-ts-artifacts.adoc[ThoughtSpot deployment artifacts for Amazon Linux 2]
> * xref:al2-install-offline.adoc[Offline Amazon Linux 2 install]
> * xref:al2-upgrade.adoc[Amazon Linux 2 upgrade]
> * xref:al2-add-node.adoc[Adding new nodes to clusters in Amazon Linux 2]
> * xref:al2-packages.adoc[Packages installed with Amazon Linux 2]