= DataFlow requirements and guidelines
:last_updated: 05/31/2022
:linkattrs:
:experimental:
:page-aliases: /data-integrate/dataflow/dataflow-best-practices.adoc
:description: Here are some requirements and guidelines for working with DataFlow.

Here are some requirements and guidelines for working with DataFlow.

== Requirements

Your organization has the following setup:

[cols="5,~",grid=none,frame=none]
|===
| &#10063; | Access ThoughtSpot Dataflow xref:dataflow-user-management-sso.adoc[using SSO].
| &#10063; | Minimum disk space allocation of approximately 5 GB in the `/etc/thoughtspot/` directory.
|===

== Guidelines

Temporary storage::
Some data sources, such as Snowflake, Amazon S3, Google Cloud Storage, and Azure Blob Storage, temporarily store data as local files before loading into the internal ThoughtSpot database.
+
These sources require additional disk space, depending on how much data is in the source.

DataFlow TQL editor::
The DataFlow TQL editor supports the following SQL commands. For details, see the xref:tql-cli-commands.adoc[TQL reference].
+
ALTER TABLE:::
+
* rename, add, and remove columns
* modify column datatypes
* add and remove primary and foreign keys.
CREATE DATABASE:::
CREATE SCHEMA:::
DELETE FROM <table> [WHERE...]:::
UPDATE <table> ... SET ... [WHERE ...]:::

&nbsp;

'''
> **Related information**
>
> * xref:tql-cli-commands.adoc[TQL reference].
