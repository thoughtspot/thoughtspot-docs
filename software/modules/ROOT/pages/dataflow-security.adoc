= DataFlow security
:last_updated: 6/8/2022
:linkattrs:
:experimental:
:description:
:description: ThoughtSpot stores data source credentials in the DataFlow metadata repository and encrypts sensitive information using AES-128.

// 7.1 and up

== Storing credentials for data sources

ThoughtSpot stores data source credentials in the DataFlow metadata repository (Postgres). We encrypt sensitive information, like passwords or authentication tokens, using AES-128. We store the tokens with the metadata.

ThoughtSpot generates the key using the Java crypto extension package classes/methods, using cipher block chaining (CBC). We store the key as part of the DataFlow code itself.

If the ThoughtSpot cluster deploys on AWS infrastructure, you can leverage AWS Vault for storing the source passwords. Currently, we support AWS vault for xref:dataflow-oracle.adoc[Oracle], xref:dataflow-netezza.adoc[Netezza], xref:dataflow-amazon-redshift.adoc[Redshift], xref:dataflow-mysql.adoc[MySQL], xref:dataflow-postgresql.adoc[PostgreSQL], and xref:dataflow-amazon-aurora.adoc[Amazon Aurora].

As of the 7.1 software release, DataFlow supports SSO and maintains only username and email id in DataFlow metadata as plain text.

NOTE: ThoughtSpot does not rotate the key; it remains static.

== DataFlow metadata location

As of the 7.0 software release, DataFlow uses ThoughtSpotâ€™s internal Postgres repository for storing the metadata for its connections.

== Securing DataFlow data in ThoughtSpot

By default, ThoughtSpot uses JDBC in _pipe_ mode for data extraction through DataFlow for all relational data sources. In this process, the system creates a named pipe under the `/export/xvdb1/large_files/diyotta_agent_stage_dir/` directory. The JDBC extraction process pushes plain data to the named pipe in chunks and in parallel, `tsload` as a service reads the chunked data from the pipe and loads it to the Falcon destination table. The source data does not persist to disk at any time. After completing the load process, the system deletes the named pipe from the staging directory.

When using xref:dataflow-snowflake.adoc[Snowflake] as the DataFlow source, the default extraction type is _bulk export_, and default extract mode is _pipe_. Here, DataFlow stages data to the Snowflake internal user stage and then exports to pipe.

You can override the extraction mode from the default _pipe_ to _file_ by using DataFlow sync properties. The JDBC process extracts all data to a `csv` file as a plain text under the  `/export/xvdb1/large_files/diyotta_agent_stage_dir/` directory, and provides the file as input to `tsload` as a service. After completing the load process, the system deletes the intermediate staging file.

With file sources like xref:dataflow-amazon-s3.adoc[Amazon S3], xref:dataflow-azure-blob-storage.adoc[Azure Blob Storage], xref:dataflow-google-cloud-storage.adoc[GCS] and xref:dataflow-hdfs.adoc[HDFS], DataFlow downloads source files to the staging directory `/export/xvdb1/large_files/diyotta_agent_stage_dir`, and sends them to `tsload` as a service. After completing the load process, the system deletes the downloaded files.


== External database encryption

ThoughtSpot supports data encryption in transit for all connection types in DataFlow. For xref:dataflow-azure-synapse.adoc[Azure Synapse], xref:dataflow-databricks-delta-lake.adoc[Databricks], xref:dataflow-google-bigquery.adoc[Google BigQuery], xref:dataflow-memsql.adoc[MemSQL managed instances], and xref:dataflow-snowflake.adoc[Snowflake], the system encrypts data by default. For more information, see xref:dataflow-security-reference.adoc[DataFlow encryption reference].
